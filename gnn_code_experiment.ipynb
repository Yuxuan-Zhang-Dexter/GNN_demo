{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuxuan-Zhang-Dexter/GNN_demo/blob/main/gnn_code_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7FRTwV4mDlE"
      },
      "source": [
        "# Dimnet in PYG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKDcoIGSl0ni"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "from functools import partial\n",
        "from math import pi as PI\n",
        "from math import sqrt\n",
        "from typing import Callable, Dict, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding, Linear\n",
        "\n",
        "from torch_geometric.data import Dataset, download_url\n",
        "from torch_geometric.nn import radius_graph\n",
        "from torch_geometric.nn.inits import glorot_orthogonal\n",
        "from torch_geometric.nn.resolver import activation_resolver\n",
        "from torch_geometric.typing import OptTensor, SparseTensor\n",
        "from torch_geometric.utils import scatter\n",
        "\n",
        "qm9_target_dict: Dict[int, str] = {\n",
        "    0: 'mu',\n",
        "    1: 'alpha',\n",
        "    2: 'homo',\n",
        "    3: 'lumo',\n",
        "    5: 'r2',\n",
        "    6: 'zpve',\n",
        "    7: 'U0',\n",
        "    8: 'U',\n",
        "    9: 'H',\n",
        "    10: 'G',\n",
        "    11: 'Cv',\n",
        "}\n",
        "\n",
        "\n",
        "class Envelope(torch.nn.Module):\n",
        "    def __init__(self, exponent: int):\n",
        "        super().__init__()\n",
        "        self.p = exponent + 1\n",
        "        self.a = -(self.p + 1) * (self.p + 2) / 2\n",
        "        self.b = self.p * (self.p + 2)\n",
        "        self.c = -self.p * (self.p + 1) / 2\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        p, a, b, c = self.p, self.a, self.b, self.c\n",
        "        x_pow_p0 = x.pow(p - 1)\n",
        "        x_pow_p1 = x_pow_p0 * x\n",
        "        x_pow_p2 = x_pow_p1 * x\n",
        "        return (1.0 / x + a * x_pow_p0 + b * x_pow_p1 +\n",
        "                c * x_pow_p2) * (x < 1.0).to(x.dtype)\n",
        "\n",
        "\n",
        "class BesselBasisLayer(torch.nn.Module):\n",
        "    def __init__(self, num_radial: int, cutoff: float = 5.0,\n",
        "                 envelope_exponent: int = 5):\n",
        "        super().__init__()\n",
        "        self.cutoff = cutoff\n",
        "        self.envelope = Envelope(envelope_exponent)\n",
        "\n",
        "        self.freq = torch.nn.Parameter(torch.empty(num_radial))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        with torch.no_grad():\n",
        "            torch.arange(1, self.freq.numel() + 1, out=self.freq).mul_(PI)\n",
        "        self.freq.requires_grad_()\n",
        "\n",
        "    def forward(self, dist: Tensor) -> Tensor:\n",
        "        dist = dist.unsqueeze(-1) / self.cutoff\n",
        "        return self.envelope(dist) * (self.freq * dist).sin()\n",
        "\n",
        "\n",
        "class SphericalBasisLayer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        cutoff: float = 5.0,\n",
        "        envelope_exponent: int = 5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        import sympy as sym\n",
        "\n",
        "        from torch_geometric.nn.models.dimenet_utils import (\n",
        "            bessel_basis,\n",
        "            real_sph_harm,\n",
        "        )\n",
        "\n",
        "        assert num_radial <= 64\n",
        "        self.num_spherical = num_spherical\n",
        "        self.num_radial = num_radial\n",
        "        self.cutoff = cutoff\n",
        "        self.envelope = Envelope(envelope_exponent)\n",
        "\n",
        "        bessel_forms = bessel_basis(num_spherical, num_radial)\n",
        "        sph_harm_forms = real_sph_harm(num_spherical)\n",
        "        self.sph_funcs = []\n",
        "        self.bessel_funcs = []\n",
        "\n",
        "        x, theta = sym.symbols('x theta')\n",
        "        modules = {'sin': torch.sin, 'cos': torch.cos}\n",
        "        for i in range(num_spherical):\n",
        "            if i == 0:\n",
        "                sph1 = sym.lambdify([theta], sph_harm_forms[i][0], modules)(0)\n",
        "                self.sph_funcs.append(partial(self._sph_to_tensor, sph1))\n",
        "            else:\n",
        "                sph = sym.lambdify([theta], sph_harm_forms[i][0], modules)\n",
        "                self.sph_funcs.append(sph)\n",
        "            for j in range(num_radial):\n",
        "                bessel = sym.lambdify([x], bessel_forms[i][j], modules)\n",
        "                self.bessel_funcs.append(bessel)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sph_to_tensor(sph, x: Tensor) -> Tensor:\n",
        "        return torch.zeros_like(x) + sph\n",
        "\n",
        "    def forward(self, dist: Tensor, angle: Tensor, idx_kj: Tensor) -> Tensor:\n",
        "        dist = dist / self.cutoff\n",
        "        rbf = torch.stack([f(dist) for f in self.bessel_funcs], dim=1)\n",
        "        rbf = self.envelope(dist).unsqueeze(-1) * rbf\n",
        "\n",
        "        cbf = torch.stack([f(angle) for f in self.sph_funcs], dim=1)\n",
        "\n",
        "        n, k = self.num_spherical, self.num_radial\n",
        "        out = (rbf[idx_kj].view(-1, n, k) * cbf.view(-1, n, 1)).view(-1, n * k)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EmbeddingBlock(torch.nn.Module):\n",
        "    ### num_radial - the number of radial basis function; hidden_channels - embedding size; act - activation function\n",
        "    def __init__(self, num_radial: int, hidden_channels: int, act: Callable):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "\n",
        "        self.emb = Embedding(95, hidden_channels) # 95 unique embeddings, embedding size\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels) # 映射 num_radial x hidden_channels matrix, From num_radial to hidden_channels\n",
        "        self.lin = Linear(3 * hidden_channels, hidden_channels) # 映射  3 * hidden_channels to hidden_channels\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self): # draw weight from a unifom distribution or a normal distribution\n",
        "        self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
        "        self.lin_rbf.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor, j: Tensor) -> Tensor: # embedding - > linear transform rbf in embedding size -> activation function ->\n",
        "                                                                              # concate node, neighbors, and transformed rbf and linear transform them to embedding size -> activation function\n",
        "        x = self.emb(x)\n",
        "        rbf = self.act(self.lin_rbf(rbf))\n",
        "        return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))\n",
        "\n",
        "\n",
        "class ResidualLayer(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels: int, act: Callable):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin1.weight, scale=2.0) # gloro_orthogonal method to initialize weight and bias = 0 in the weight w0\n",
        "        self.lin1.bias.data.fill_(0)\n",
        "        glorot_orthogonal(self.lin2.weight, scale=2.0)\n",
        "        self.lin2.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return x + self.act(self.lin2(self.act(self.lin1(x)))) # 残差连接，x + learned weight from mlp\n",
        "\n",
        "\n",
        "class InteractionBlock(torch.nn.Module):\n",
        "    # num_bilinear (int) – Size of the bilinear layer tensor; num_spherical (int) – Number of spherical harmonics.\n",
        "    # num_before_skip (int, optional) – Number of residual layers in the interaction blocks before the skip connection. (default: 1)\n",
        "    # num_after_skip (int, optional) – Number of residual layers in the interaction blocks after the skip connection. (default: 2)\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        num_bilinear: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        num_before_skip: int,\n",
        "        num_after_skip: int,\n",
        "        act: Callable,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False) ### num_radial to hidden_channels\n",
        "        self.lin_sbf = Linear(num_spherical * num_radial, num_bilinear, ### num_spherical * num_radial to num_bilinear\n",
        "                              bias=False)\n",
        "\n",
        "        # Dense transformations of input messages.\n",
        "        self.lin_kj = Linear(hidden_channels, hidden_channels) # set\n",
        "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.W = torch.nn.Parameter(\n",
        "            torch.empty(hidden_channels, num_bilinear, hidden_channels)) # hidden_channels * num_bilinear * hidden_channels tensor as  parameter\n",
        "\n",
        "        self.layers_before_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip) # run residual layers before skip connection\n",
        "        ])\n",
        "        self.lin = Linear(hidden_channels, hidden_channels)\n",
        "        self.layers_after_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_after_skip) # run residual layers after skip connection\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_sbf.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
        "        self.lin_kj.bias.data.fill_(0)\n",
        "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
        "        self.lin_ji.bias.data.fill_(0)\n",
        "        self.W.data.normal_(mean=0, std=2 / self.W.size(0))\n",
        "        for res_layer in self.layers_before_skip:\n",
        "            res_layer.reset_parameters()\n",
        "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "        self.lin.bias.data.fill_(0)\n",
        "        for res_layer in self.layers_after_skip:\n",
        "            res_layer.reset_parameters()\n",
        "\n",
        "    # assuming x_ji = num_node x hidden_channels representing connections between node and neighbors; x_kj = num_node x hidden_channels representing connections between neighbor and neighbors' neighbors;\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
        "                idx_ji: Tensor) -> Tensor:\n",
        "        rbf = self.lin_rbf(rbf) ### num_radial to hidden_channels:  num_node x hidden_channels\n",
        "        sbf = self.lin_sbf(sbf) ### num_spherical * num_radial to num_bilinear\n",
        "\n",
        "        x_ji = self.act(self.lin_ji(x)) ### hidden to hidden - connections between ith node and all jth neighbors\n",
        "        x_kj = self.act(self.lin_kj(x)) ### hidden to hidden - connections between jth neighbor and all kth neighbor's neighbors\n",
        "        x_kj = x_kj * rbf ### element-wise multiplication each embedding\n",
        "        ### batch multiplication, j = num_bilinear; l = hidden embedding ; wjl and ijl, element-wise multiplcation, sum over i j\n",
        "        ### idx_kj should be indexes of nodes are neighbors\n",
        "        x_kj = torch.einsum('wj,wl,ijl->wi', sbf, x_kj[idx_kj], self.W)   ### wi - num_node x hidden channels\n",
        "        ### idx_ji represent connections between neighbor and neighbors' neighbor\n",
        "        ### we aggregate all neighbors'neighbors' message into neighbors\n",
        "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0), reduce='sum')\n",
        "        ### the new embedding = the ith node embeddings + the aggregated jth nod embeddings\n",
        "        h = x_ji + x_kj\n",
        "        for layer in self.layers_before_skip:\n",
        "            h = layer(h)\n",
        "        h = self.act(self.lin(h)) + x\n",
        "        for layer in self.layers_after_skip:\n",
        "            h = layer(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class InteractionPPBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        int_emb_size: int,\n",
        "        basis_emb_size: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        num_before_skip: int,\n",
        "        num_after_skip: int,\n",
        "        act: Callable,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "\n",
        "        # Transformation of Bessel and spherical basis representations:\n",
        "        self.lin_rbf1 = Linear(num_radial, basis_emb_size, bias=False)\n",
        "        self.lin_rbf2 = Linear(basis_emb_size, hidden_channels, bias=False)\n",
        "\n",
        "        self.lin_sbf1 = Linear(num_spherical * num_radial, basis_emb_size,\n",
        "                               bias=False)\n",
        "        self.lin_sbf2 = Linear(basis_emb_size, int_emb_size, bias=False)\n",
        "\n",
        "        # Hidden transformation of input message:\n",
        "        self.lin_kj = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Embedding projections for interaction triplets:\n",
        "        self.lin_down = Linear(hidden_channels, int_emb_size, bias=False)\n",
        "        self.lin_up = Linear(int_emb_size, hidden_channels, bias=False)\n",
        "\n",
        "        # Residual layers before and after skip connection:\n",
        "        self.layers_before_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
        "        ])\n",
        "        self.lin = Linear(hidden_channels, hidden_channels)\n",
        "        self.layers_after_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_after_skip)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf1.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_rbf2.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_sbf1.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_sbf2.weight, scale=2.0)\n",
        "\n",
        "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
        "        self.lin_kj.bias.data.fill_(0)\n",
        "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
        "        self.lin_ji.bias.data.fill_(0)\n",
        "\n",
        "        glorot_orthogonal(self.lin_down.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
        "\n",
        "        for res_layer in self.layers_before_skip:\n",
        "            res_layer.reset_parameters()\n",
        "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "        self.lin.bias.data.fill_(0)\n",
        "        for res_layer in self.layers_after_skip:\n",
        "            res_layer.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
        "                idx_ji: Tensor) -> Tensor:\n",
        "        # Initial transformation:\n",
        "        x_ji = self.act(self.lin_ji(x))\n",
        "        x_kj = self.act(self.lin_kj(x))\n",
        "\n",
        "        # Transformation via Bessel basis:\n",
        "        rbf = self.lin_rbf1(rbf)\n",
        "        rbf = self.lin_rbf2(rbf)\n",
        "        x_kj = x_kj * rbf\n",
        "\n",
        "        # Down project embedding and generating triple-interactions:\n",
        "        x_kj = self.act(self.lin_down(x_kj))\n",
        "\n",
        "        # Transform via 2D spherical basis:\n",
        "        sbf = self.lin_sbf1(sbf)\n",
        "        sbf = self.lin_sbf2(sbf)\n",
        "        x_kj = x_kj[idx_kj] * sbf\n",
        "\n",
        "        # Aggregate interactions and up-project embeddings:\n",
        "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0), reduce='sum')\n",
        "        x_kj = self.act(self.lin_up(x_kj))\n",
        "\n",
        "        h = x_ji + x_kj\n",
        "        for layer in self.layers_before_skip:\n",
        "            h = layer(h)\n",
        "        h = self.act(self.lin(h)) + x\n",
        "        for layer in self.layers_after_skip:\n",
        "            h = layer(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class OutputBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_radial: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        num_layers: int,\n",
        "        act: Callable,\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        assert output_initializer in {'zeros', 'glorot_orthogonal'}\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.act = act\n",
        "        self.output_initializer = output_initializer\n",
        "\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.lins.append(Linear(hidden_channels, hidden_channels))\n",
        "        self.lin = Linear(hidden_channels, out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
        "        for lin in self.lins:\n",
        "            glorot_orthogonal(lin.weight, scale=2.0)\n",
        "            lin.bias.data.fill_(0)\n",
        "        if self.output_initializer == 'zeros':\n",
        "            self.lin.weight.data.fill_(0)\n",
        "        elif self.output_initializer == 'glorot_orthogonal':\n",
        "            glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
        "                num_nodes: Optional[int] = None) -> Tensor:\n",
        "        x = self.lin_rbf(rbf) * x\n",
        "        x = scatter(x, i, dim=0, dim_size=num_nodes, reduce='sum')\n",
        "        for lin in self.lins:\n",
        "            x = self.act(lin(x))\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "class OutputPPBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_radial: int,\n",
        "        hidden_channels: int,\n",
        "        out_emb_channels: int,\n",
        "        out_channels: int,\n",
        "        num_layers: int,\n",
        "        act: Callable,\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        assert output_initializer in {'zeros', 'glorot_orthogonal'}\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.act = act\n",
        "        self.output_initializer = output_initializer\n",
        "\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
        "\n",
        "        # The up-projection layer:\n",
        "        self.lin_up = Linear(hidden_channels, out_emb_channels, bias=False)\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.lins.append(Linear(out_emb_channels, out_emb_channels))\n",
        "        self.lin = Linear(out_emb_channels, out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
        "        for lin in self.lins:\n",
        "            glorot_orthogonal(lin.weight, scale=2.0)\n",
        "            lin.bias.data.fill_(0)\n",
        "        if self.output_initializer == 'zeros':\n",
        "            self.lin.weight.data.fill_(0)\n",
        "        elif self.output_initializer == 'glorot_orthogonal':\n",
        "            glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
        "                num_nodes: Optional[int] = None) -> Tensor:\n",
        "        x = self.lin_rbf(rbf) * x\n",
        "        x = scatter(x, i, dim=0, dim_size=num_nodes, reduce='sum')\n",
        "        x = self.lin_up(x)\n",
        "        for lin in self.lins:\n",
        "            x = self.act(lin(x))\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "def triplets(\n",
        "    edge_index: Tensor,\n",
        "    num_nodes: int,\n",
        ") -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
        "    row, col = edge_index  # j->i\n",
        "\n",
        "    value = torch.arange(row.size(0), device=row.device)\n",
        "    adj_t = SparseTensor(row=col, col=row, value=value,\n",
        "                         sparse_sizes=(num_nodes, num_nodes))\n",
        "    adj_t_row = adj_t[row]\n",
        "    num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
        "\n",
        "    # Node indices (k->j->i) for triplets.\n",
        "    idx_i = col.repeat_interleave(num_triplets)\n",
        "    idx_j = row.repeat_interleave(num_triplets)\n",
        "    idx_k = adj_t_row.storage.col()\n",
        "    mask = idx_i != idx_k  # Remove i == k triplets.\n",
        "    idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n",
        "\n",
        "    # Edge indices (k-j, j->i) for triplets.\n",
        "    idx_kj = adj_t_row.storage.value()[mask]\n",
        "    idx_ji = adj_t_row.storage.row()[mask]\n",
        "\n",
        "    return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji\n",
        "\n",
        "\n",
        "[docs]class DimeNet(torch.nn.Module):\n",
        "    r\"\"\"The directional message passing neural network (DimeNet) from the\n",
        "    `\"Directional Message Passing for Molecular Graphs\"\n",
        "    <https://arxiv.org/abs/2003.03123>`_ paper.\n",
        "    DimeNet transforms messages based on the angle between them in a\n",
        "    rotation-equivariant fashion.\n",
        "\n",
        "    .. note::\n",
        "\n",
        "        For an example of using a pretrained DimeNet variant, see\n",
        "        `examples/qm9_pretrained_dimenet.py\n",
        "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
        "        qm9_pretrained_dimenet.py>`_.\n",
        "\n",
        "    Args:\n",
        "        hidden_channels (int): Hidden embedding size.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_blocks (int): Number of building blocks.\n",
        "        num_bilinear (int): Size of the bilinear layer tensor.\n",
        "        num_spherical (int): Number of spherical harmonics.\n",
        "        num_radial (int): Number of radial basis functions.\n",
        "        cutoff (float, optional): Cutoff distance for interatomic\n",
        "            interactions. (default: :obj:`5.0`)\n",
        "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
        "            collect for each node within the :attr:`cutoff` distance.\n",
        "            (default: :obj:`32`)\n",
        "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
        "            (default: :obj:`5`)\n",
        "        num_before_skip (int, optional): Number of residual layers in the\n",
        "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
        "        num_after_skip (int, optional): Number of residual layers in the\n",
        "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
        "        num_output_layers (int, optional): Number of linear layers for the\n",
        "            output blocks. (default: :obj:`3`)\n",
        "        act (str or Callable, optional): The activation function.\n",
        "            (default: :obj:`\"swish\"`)\n",
        "        output_initializer (str, optional): The initialization method for the\n",
        "            output layer (:obj:`\"zeros\"`, :obj:`\"glorot_orthogonal\"`).\n",
        "            (default: :obj:`\"zeros\"`)\n",
        "    \"\"\"\n",
        "\n",
        "    url = ('https://github.com/klicperajo/dimenet/raw/master/pretrained/'\n",
        "           'dimenet')\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        num_blocks: int,\n",
        "        num_bilinear: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        cutoff: float = 5.0,\n",
        "        max_num_neighbors: int = 32,\n",
        "        envelope_exponent: int = 5,\n",
        "        num_before_skip: int = 1,\n",
        "        num_after_skip: int = 2,\n",
        "        num_output_layers: int = 3,\n",
        "        act: Union[str, Callable] = 'swish',\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_spherical < 2:\n",
        "            raise ValueError(\"'num_spherical' should be greater than 1\")\n",
        "\n",
        "        act = activation_resolver(act)\n",
        "\n",
        "        self.cutoff = cutoff\n",
        "        self.max_num_neighbors = max_num_neighbors\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        self.rbf = BesselBasisLayer(num_radial, cutoff, envelope_exponent)\n",
        "        self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n",
        "                                       envelope_exponent)\n",
        "\n",
        "        self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n",
        "\n",
        "        self.output_blocks = torch.nn.ModuleList([\n",
        "            OutputBlock(\n",
        "                num_radial,\n",
        "                hidden_channels,\n",
        "                out_channels,\n",
        "                num_output_layers,\n",
        "                act,\n",
        "                output_initializer,\n",
        "            ) for _ in range(num_blocks + 1)\n",
        "        ])\n",
        "\n",
        "        self.interaction_blocks = torch.nn.ModuleList([\n",
        "            InteractionBlock(\n",
        "                hidden_channels,\n",
        "                num_bilinear,\n",
        "                num_spherical,\n",
        "                num_radial,\n",
        "                num_before_skip,\n",
        "                num_after_skip,\n",
        "                act,\n",
        "            ) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "[docs]    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        self.rbf.reset_parameters()\n",
        "        self.emb.reset_parameters()\n",
        "        for out in self.output_blocks:\n",
        "            out.reset_parameters()\n",
        "        for interaction in self.interaction_blocks:\n",
        "            interaction.reset_parameters()\n",
        "\n",
        "[docs]    @classmethod\n",
        "    def from_qm9_pretrained(\n",
        "        cls,\n",
        "        root: str,\n",
        "        dataset: Dataset,\n",
        "        target: int,\n",
        "    ) -> Tuple['DimeNet', Dataset, Dataset, Dataset]:  # pragma: no cover\n",
        "        r\"\"\"Returns a pre-trained :class:`DimeNet` model on the\n",
        "        :class:`~torch_geometric.datasets.QM9` dataset, trained on the\n",
        "        specified target :obj:`target`.\n",
        "        \"\"\"\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "        import tensorflow as tf\n",
        "\n",
        "        assert target >= 0 and target <= 12 and not target == 4\n",
        "\n",
        "        root = osp.expanduser(osp.normpath(root))\n",
        "        path = osp.join(root, 'pretrained_dimenet', qm9_target_dict[target])\n",
        "\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        url = f'{cls.url}/{qm9_target_dict[target]}'\n",
        "\n",
        "        if not osp.exists(osp.join(path, 'checkpoint')):\n",
        "            download_url(f'{url}/checkpoint', path)\n",
        "            download_url(f'{url}/ckpt.data-00000-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.data-00001-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.index', path)\n",
        "\n",
        "        path = osp.join(path, 'ckpt')\n",
        "        reader = tf.train.load_checkpoint(path)\n",
        "\n",
        "        model = cls(\n",
        "            hidden_channels=128,\n",
        "            out_channels=1,\n",
        "            num_blocks=6,\n",
        "            num_bilinear=8,\n",
        "            num_spherical=7,\n",
        "            num_radial=6,\n",
        "            cutoff=5.0,\n",
        "            envelope_exponent=5,\n",
        "            num_before_skip=1,\n",
        "            num_after_skip=2,\n",
        "            num_output_layers=3,\n",
        "        )\n",
        "\n",
        "        def copy_(src, name, transpose=False):\n",
        "            init = reader.get_tensor(f'{name}/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "            init = torch.from_numpy(init)\n",
        "            if name[-6:] == 'kernel':\n",
        "                init = init.t()\n",
        "            src.data.copy_(init)\n",
        "\n",
        "        copy_(model.rbf.freq, 'rbf_layer/frequencies')\n",
        "        copy_(model.emb.emb.weight, 'emb_block/embeddings')\n",
        "        copy_(model.emb.lin_rbf.weight, 'emb_block/dense_rbf/kernel')\n",
        "        copy_(model.emb.lin_rbf.bias, 'emb_block/dense_rbf/bias')\n",
        "        copy_(model.emb.lin.weight, 'emb_block/dense/kernel')\n",
        "        copy_(model.emb.lin.bias, 'emb_block/dense/bias')\n",
        "\n",
        "        for i, block in enumerate(model.output_blocks):\n",
        "            copy_(block.lin_rbf.weight, f'output_blocks/{i}/dense_rbf/kernel')\n",
        "            for j, lin in enumerate(block.lins):\n",
        "                copy_(lin.weight, f'output_blocks/{i}/dense_layers/{j}/kernel')\n",
        "                copy_(lin.bias, f'output_blocks/{i}/dense_layers/{j}/bias')\n",
        "            copy_(block.lin.weight, f'output_blocks/{i}/dense_final/kernel')\n",
        "\n",
        "        for i, block in enumerate(model.interaction_blocks):\n",
        "            copy_(block.lin_rbf.weight, f'int_blocks/{i}/dense_rbf/kernel')\n",
        "            copy_(block.lin_sbf.weight, f'int_blocks/{i}/dense_sbf/kernel')\n",
        "            copy_(block.lin_kj.weight, f'int_blocks/{i}/dense_kj/kernel')\n",
        "            copy_(block.lin_kj.bias, f'int_blocks/{i}/dense_kj/bias')\n",
        "            copy_(block.lin_ji.weight, f'int_blocks/{i}/dense_ji/kernel')\n",
        "            copy_(block.lin_ji.bias, f'int_blocks/{i}/dense_ji/bias')\n",
        "            copy_(block.W, f'int_blocks/{i}/bilinear')\n",
        "            for j, layer in enumerate(block.layers_before_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/bias')\n",
        "            copy_(block.lin.weight, f'int_blocks/{i}/final_before_skip/kernel')\n",
        "            copy_(block.lin.bias, f'int_blocks/{i}/final_before_skip/bias')\n",
        "            for j, layer in enumerate(block.layers_after_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/bias')\n",
        "\n",
        "        # Use the same random seed as the official DimeNet` implementation.\n",
        "        random_state = np.random.RandomState(seed=42)\n",
        "        perm = torch.from_numpy(random_state.permutation(np.arange(130831)))\n",
        "        perm = perm.long()\n",
        "        train_idx = perm[:110000]\n",
        "        val_idx = perm[110000:120000]\n",
        "        test_idx = perm[120000:]\n",
        "\n",
        "        return model, (dataset[train_idx], dataset[val_idx], dataset[test_idx])\n",
        "\n",
        "[docs]    def forward(\n",
        "        self,\n",
        "        z: Tensor,\n",
        "        pos: Tensor,\n",
        "        batch: OptTensor = None,\n",
        "    ) -> Tensor:\n",
        "        r\"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            z (torch.Tensor): Atomic number of each atom with shape\n",
        "                :obj:`[num_atoms]`.\n",
        "            pos (torch.Tensor): Coordinates of each atom with shape\n",
        "                :obj:`[num_atoms, 3]`.\n",
        "            batch (torch.Tensor, optional): Batch indices assigning each atom\n",
        "                to a separate molecule with shape :obj:`[num_atoms]`.\n",
        "                (default: :obj:`None`)\n",
        "        \"\"\"\n",
        "        edge_index = radius_graph(pos, r=self.cutoff, batch=batch,\n",
        "                                  max_num_neighbors=self.max_num_neighbors)\n",
        "\n",
        "        i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = triplets(\n",
        "            edge_index, num_nodes=z.size(0))\n",
        "\n",
        "        # Calculate distances.\n",
        "        dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n",
        "\n",
        "        # Calculate angles.\n",
        "        if isinstance(self, DimeNetPlusPlus):\n",
        "            pos_jk, pos_ij = pos[idx_j] - pos[idx_k], pos[idx_i] - pos[idx_j]\n",
        "            a = (pos_ij * pos_jk).sum(dim=-1)\n",
        "            b = torch.cross(pos_ij, pos_jk).norm(dim=-1)\n",
        "        elif isinstance(self, DimeNet):\n",
        "            pos_ji, pos_ki = pos[idx_j] - pos[idx_i], pos[idx_k] - pos[idx_i]\n",
        "            a = (pos_ji * pos_ki).sum(dim=-1)\n",
        "            b = torch.cross(pos_ji, pos_ki).norm(dim=-1)\n",
        "        angle = torch.atan2(b, a)\n",
        "\n",
        "        rbf = self.rbf(dist)\n",
        "        sbf = self.sbf(dist, angle, idx_kj)\n",
        "\n",
        "        # Embedding block.\n",
        "        x = self.emb(z, rbf, i, j)\n",
        "        P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n",
        "\n",
        "        # Interaction blocks.\n",
        "        for interaction_block, output_block in zip(self.interaction_blocks,\n",
        "                                                   self.output_blocks[1:]):\n",
        "            x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
        "            P = P + output_block(x, rbf, i, num_nodes=pos.size(0))\n",
        "\n",
        "        if batch is None:\n",
        "            return P.sum(dim=0)\n",
        "        else:\n",
        "            return scatter(P, batch, dim=0, reduce='sum')\n",
        "\n",
        "\n",
        "[docs]class DimeNetPlusPlus(DimeNet):\n",
        "    r\"\"\"The DimeNet++ from the `\"Fast and Uncertainty-Aware\n",
        "    Directional Message Passing for Non-Equilibrium Molecules\"\n",
        "    <https://arxiv.org/abs/2011.14115>`_ paper.\n",
        "\n",
        "    :class:`DimeNetPlusPlus` is an upgrade to the :class:`DimeNet` model with\n",
        "    8x faster and 10% more accurate than :class:`DimeNet`.\n",
        "\n",
        "    Args:\n",
        "        hidden_channels (int): Hidden embedding size.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_blocks (int): Number of building blocks.\n",
        "        int_emb_size (int): Size of embedding in the interaction block.\n",
        "        basis_emb_size (int): Size of basis embedding in the interaction block.\n",
        "        out_emb_channels (int): Size of embedding in the output block.\n",
        "        num_spherical (int): Number of spherical harmonics.\n",
        "        num_radial (int): Number of radial basis functions.\n",
        "        cutoff: (float, optional): Cutoff distance for interatomic\n",
        "            interactions. (default: :obj:`5.0`)\n",
        "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
        "            collect for each node within the :attr:`cutoff` distance.\n",
        "            (default: :obj:`32`)\n",
        "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
        "            (default: :obj:`5`)\n",
        "        num_before_skip: (int, optional): Number of residual layers in the\n",
        "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
        "        num_after_skip: (int, optional): Number of residual layers in the\n",
        "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
        "        num_output_layers: (int, optional): Number of linear layers for the\n",
        "            output blocks. (default: :obj:`3`)\n",
        "        act: (str or Callable, optional): The activation funtion.\n",
        "            (default: :obj:`\"swish\"`)\n",
        "        output_initializer (str, optional): The initialization method for the\n",
        "            output layer (:obj:`\"zeros\"`, :obj:`\"glorot_orthogonal\"`).\n",
        "            (default: :obj:`\"zeros\"`)\n",
        "    \"\"\"\n",
        "\n",
        "    url = ('https://raw.githubusercontent.com/gasteigerjo/dimenet/'\n",
        "           'master/pretrained/dimenet_pp')\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        num_blocks: int,\n",
        "        int_emb_size: int,\n",
        "        basis_emb_size: int,\n",
        "        out_emb_channels: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        cutoff: float = 5.0,\n",
        "        max_num_neighbors: int = 32,\n",
        "        envelope_exponent: int = 5,\n",
        "        num_before_skip: int = 1,\n",
        "        num_after_skip: int = 2,\n",
        "        num_output_layers: int = 3,\n",
        "        act: Union[str, Callable] = 'swish',\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        act = activation_resolver(act)\n",
        "\n",
        "        super().__init__(\n",
        "            hidden_channels=hidden_channels,\n",
        "            out_channels=out_channels,\n",
        "            num_blocks=num_blocks,\n",
        "            num_bilinear=1,\n",
        "            num_spherical=num_spherical,\n",
        "            num_radial=num_radial,\n",
        "            cutoff=cutoff,\n",
        "            max_num_neighbors=max_num_neighbors,\n",
        "            envelope_exponent=envelope_exponent,\n",
        "            num_before_skip=num_before_skip,\n",
        "            num_after_skip=num_after_skip,\n",
        "            num_output_layers=num_output_layers,\n",
        "            act=act,\n",
        "            output_initializer=output_initializer,\n",
        "        )\n",
        "\n",
        "        # We are re-using the RBF, SBF and embedding layers of `DimeNet` and\n",
        "        # redefine output_block and interaction_block in DimeNet++.\n",
        "        # Hence, it is to be noted that in the above initalization, the\n",
        "        # variable `num_bilinear` does not have any purpose as it is used\n",
        "        # solely in the `OutputBlock` of DimeNet:\n",
        "        self.output_blocks = torch.nn.ModuleList([\n",
        "            OutputPPBlock(\n",
        "                num_radial,\n",
        "                hidden_channels,\n",
        "                out_emb_channels,\n",
        "                out_channels,\n",
        "                num_output_layers,\n",
        "                act,\n",
        "                output_initializer,\n",
        "            ) for _ in range(num_blocks + 1)\n",
        "        ])\n",
        "\n",
        "        self.interaction_blocks = torch.nn.ModuleList([\n",
        "            InteractionPPBlock(\n",
        "                hidden_channels,\n",
        "                int_emb_size,\n",
        "                basis_emb_size,\n",
        "                num_spherical,\n",
        "                num_radial,\n",
        "                num_before_skip,\n",
        "                num_after_skip,\n",
        "                act,\n",
        "            ) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "[docs]    @classmethod\n",
        "    def from_qm9_pretrained(\n",
        "        cls,\n",
        "        root: str,\n",
        "        dataset: Dataset,\n",
        "        target: int,\n",
        "    ) -> Tuple['DimeNetPlusPlus', Dataset, Dataset,\n",
        "               Dataset]:  # pragma: no cover\n",
        "        r\"\"\"Returns a pre-trained :class:`DimeNetPlusPlus` model on the\n",
        "        :class:`~torch_geometric.datasets.QM9` dataset, trained on the\n",
        "        specified target :obj:`target`.\n",
        "        \"\"\"\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "        import tensorflow as tf\n",
        "\n",
        "        assert target >= 0 and target <= 12 and not target == 4\n",
        "\n",
        "        root = osp.expanduser(osp.normpath(root))\n",
        "        path = osp.join(root, 'pretrained_dimenet_pp', qm9_target_dict[target])\n",
        "\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        url = f'{cls.url}/{qm9_target_dict[target]}'\n",
        "\n",
        "        if not osp.exists(osp.join(path, 'checkpoint')):\n",
        "            download_url(f'{url}/checkpoint', path)\n",
        "            download_url(f'{url}/ckpt.data-00000-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.data-00001-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.index', path)\n",
        "\n",
        "        path = osp.join(path, 'ckpt')\n",
        "        reader = tf.train.load_checkpoint(path)\n",
        "\n",
        "        # Configuration from DimeNet++:\n",
        "        # https://github.com/gasteigerjo/dimenet/blob/master/config_pp.yaml\n",
        "        model = cls(\n",
        "            hidden_channels=128,\n",
        "            out_channels=1,\n",
        "            num_blocks=4,\n",
        "            int_emb_size=64,\n",
        "            basis_emb_size=8,\n",
        "            out_emb_channels=256,\n",
        "            num_spherical=7,\n",
        "            num_radial=6,\n",
        "            cutoff=5.0,\n",
        "            max_num_neighbors=32,\n",
        "            envelope_exponent=5,\n",
        "            num_before_skip=1,\n",
        "            num_after_skip=2,\n",
        "            num_output_layers=3,\n",
        "        )\n",
        "\n",
        "        def copy_(src, name, transpose=False):\n",
        "            init = reader.get_tensor(f'{name}/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "            init = torch.from_numpy(init)\n",
        "            if name[-6:] == 'kernel':\n",
        "                init = init.t()\n",
        "            src.data.copy_(init)\n",
        "\n",
        "        copy_(model.rbf.freq, 'rbf_layer/frequencies')\n",
        "        copy_(model.emb.emb.weight, 'emb_block/embeddings')\n",
        "        copy_(model.emb.lin_rbf.weight, 'emb_block/dense_rbf/kernel')\n",
        "        copy_(model.emb.lin_rbf.bias, 'emb_block/dense_rbf/bias')\n",
        "        copy_(model.emb.lin.weight, 'emb_block/dense/kernel')\n",
        "        copy_(model.emb.lin.bias, 'emb_block/dense/bias')\n",
        "\n",
        "        for i, block in enumerate(model.output_blocks):\n",
        "            copy_(block.lin_rbf.weight, f'output_blocks/{i}/dense_rbf/kernel')\n",
        "            copy_(block.lin_up.weight,\n",
        "                  f'output_blocks/{i}/up_projection/kernel')\n",
        "            for j, lin in enumerate(block.lins):\n",
        "                copy_(lin.weight, f'output_blocks/{i}/dense_layers/{j}/kernel')\n",
        "                copy_(lin.bias, f'output_blocks/{i}/dense_layers/{j}/bias')\n",
        "            copy_(block.lin.weight, f'output_blocks/{i}/dense_final/kernel')\n",
        "\n",
        "        for i, block in enumerate(model.interaction_blocks):\n",
        "            copy_(block.lin_rbf1.weight, f'int_blocks/{i}/dense_rbf1/kernel')\n",
        "            copy_(block.lin_rbf2.weight, f'int_blocks/{i}/dense_rbf2/kernel')\n",
        "            copy_(block.lin_sbf1.weight, f'int_blocks/{i}/dense_sbf1/kernel')\n",
        "            copy_(block.lin_sbf2.weight, f'int_blocks/{i}/dense_sbf2/kernel')\n",
        "\n",
        "            copy_(block.lin_ji.weight, f'int_blocks/{i}/dense_ji/kernel')\n",
        "            copy_(block.lin_ji.bias, f'int_blocks/{i}/dense_ji/bias')\n",
        "            copy_(block.lin_kj.weight, f'int_blocks/{i}/dense_kj/kernel')\n",
        "            copy_(block.lin_kj.bias, f'int_blocks/{i}/dense_kj/bias')\n",
        "\n",
        "            copy_(block.lin_down.weight,\n",
        "                  f'int_blocks/{i}/down_projection/kernel')\n",
        "            copy_(block.lin_up.weight, f'int_blocks/{i}/up_projection/kernel')\n",
        "\n",
        "            for j, layer in enumerate(block.layers_before_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/bias')\n",
        "\n",
        "            copy_(block.lin.weight, f'int_blocks/{i}/final_before_skip/kernel')\n",
        "            copy_(block.lin.bias, f'int_blocks/{i}/final_before_skip/bias')\n",
        "\n",
        "            for j, layer in enumerate(block.layers_after_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/bias')\n",
        "\n",
        "        random_state = np.random.RandomState(seed=42)\n",
        "        perm = torch.from_numpy(random_state.permutation(np.arange(130831)))\n",
        "        perm = perm.long()\n",
        "        train_idx = perm[:110000]\n",
        "        val_idx = perm[110000:120000]\n",
        "        test_idx = perm[120000:]\n",
        "\n",
        "        return model, (dataset[train_idx], dataset[val_idx], dataset[test_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "SRBRVIQ2DzTP",
        "outputId": "04509a89-a4f7-4082-f5ca-bae2f2a2a015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10]])\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'scatter_' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a24a35bf86e5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'scatter_' is not defined"
          ]
        }
      ],
      "source": [
        "src = torch.arange(1, 11).reshape((2, 5))\n",
        "print(src)\n",
        "index = torch.tensor([[0, 1, 2, 0]])\n",
        "scatter_(0, index, src)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxzI4swbB5oq",
        "outputId": "a42d3bbc-3a45-4ebc-e557-8604d116d18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHBxxFx2ktCM",
        "outputId": "a69cd1c6-c86f-4abb-96f2-1293ce6aebe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.10/dist-packages (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPi_zlCDR4yC",
        "outputId": "7eb7e2a1-0ab8-44a3-c04b-9e7e128e4eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 6, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.9514,  0.8095,  0.5480,  ..., -3.4367, -0.1673, -1.0950],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [-1.3129, -0.7874,  0.2084,  ..., -0.3517, -0.6588,  0.5150],\n",
              "         [ 1.2098,  0.8840,  2.7766,  ...,  1.7667,  2.1605, -1.5863],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 0.1129, -2.8237,  1.6057,  ..., -0.3822, -0.1311, -1.0600],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [-0.6204, -0.9251,  0.3499,  ..., -0.2725, -0.5474,  0.4963],\n",
              "         [ 3.6448,  0.5932, -0.1309,  ...,  1.7350,  1.3479,  2.1081],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[ 1.1263, -0.1374, -2.5185,  ...,  1.0591,  2.0035,  2.1441],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 1.2049,  0.7861,  0.0563,  ..., -0.8006, -0.3874,  0.0266],\n",
              "         [ 0.9705, -0.6846,  0.3681,  ..., -1.1811,  1.0122,  0.0204],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.6252,  0.9792,  1.6160,  ..., -0.9716,  0.7743, -0.4332],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [-1.5331,  0.4997, -1.3558,  ..., -0.9564,  0.1601, -0.1234],\n",
              "         [ 2.1271, -1.3808,  0.9606,  ..., -2.0697, -1.7597,  2.9627],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[-2.0679,  1.3003,  1.8333,  ...,  1.7219, -0.7362,  1.8370],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.6122, -2.1978, -0.5675,  ..., -0.9725, -0.3407, -1.3643],\n",
              "         [ 0.5182,  3.2804,  0.4727,  ..., -0.5077, -0.1970,  0.0897],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "        [[-0.2722,  2.8388, -1.3035,  ..., -3.5449,  4.6911, -0.3963],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.3802,  0.8579,  0.8825,  ...,  0.0937,  0.9490, -1.8178],\n",
              "         [ 1.9959,  0.3660,  0.1725,  ...,  2.6425, -1.8532, -1.9730],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from torch_scatter import scatter\n",
        "import torch\n",
        "\n",
        "src = torch.randn(10, 6, 64)\n",
        "index = torch.tensor([0, 3, 0, 3, 2, 3])\n",
        "\n",
        "# Broadcasting in the first and last dim.\n",
        "out = scatter(src, index, dim=1, dim_size = 6, reduce=\"sum\")\n",
        "\n",
        "print(out.size())\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Analysis"
      ],
      "metadata": {
        "id": "yLJkhfjEbOOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCvTludAbXh5",
        "outputId": "698ac53d-ce64-4faf-816e-a44269341915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TUDataset\n",
        "#### Social network\n",
        "from torch_geometric.datasets import TUDataset\n",
        "dataset = TUDataset(root = \"Social networks\", name = 'facebook_ct1')\n",
        "dataset[0].x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "4c7ZWHhfbSNj",
        "outputId": "588c35cc-3565-4835-a771-adb61ad767d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected sequence of length 2 at dim 1 (got 4)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ad211a0377e4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#### Social network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTUDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTUDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Social networks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'facebook_ct1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/datasets/tu_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, name, transform, pre_transform, pre_filter, use_node_attr, use_edge_attr, cleaned)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     ):\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pre_transform.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/datasets/tu_dataset.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_tu_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_filter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/io/tu.py\u001b[0m in \u001b[0;36mread_tu_data\u001b[0;34m(folder, prefix)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mnode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'node_labels'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mnode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'node_labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mnode_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/io/tu.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(folder, prefix, name, dtype)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{prefix}_{name}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mread_txt_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/io/txt_array.py\u001b[0m in \u001b[0;36mread_txt_array\u001b[0;34m(path, sep, start, end, dtype, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparse_txt_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/io/txt_array.py\u001b[0m in \u001b[0;36mparse_txt_array\u001b[0;34m(src, sep, start, end, dtype, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "id": "dRXeQ_i9fZeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4aa934-e9bf-4a3d-a8c0-ef0e524d73a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-1.1.3-cp310-cp310-manylinux1_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.11.17)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.data import FB15kDataset\n",
        "\n",
        "dataset = FB15kDataset(reverse=True, raw_dir=None, force_reload=False, verbose=True, transform=None)\n"
      ],
      "metadata": {
        "id": "hzcT-1utbKyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb8c694-bb8b-439c-eaa3-0d470a0e5ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# entities: 14951\n",
            "# relations: 1345\n",
            "# training edges: 483142\n",
            "# validation edges: 50000\n",
            "# testing edges: 59071\n",
            "Done loading data from cached files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import dgl\n",
        "for g in dataset:\n",
        "    # Convert DGL Graph to NetworkX graph for visualization\n",
        "    nx_g = dgl.to_networkx(g)\n",
        "\n",
        "    # Draw the graph\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    nx.draw(nx_g, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)\n",
        "    plt.show()\n",
        "\n",
        "    # To create and visualize an adjacency matrix\n",
        "    adj_matrix = g.adjacency_matrix()\n",
        "    plt.imshow(adj_matrix.to_dense(), cmap=\"YlGn\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EXY8hr0PpTsS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP40R3Y5qtPjRBzPhqL9el",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}