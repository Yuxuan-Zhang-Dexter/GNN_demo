{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjA5tQafSWSC88mQqOqeY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuxuan-Zhang-Dexter/GNN_demo/blob/main/gnn_code_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKDcoIGSl0ni"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "from functools import partial\n",
        "from math import pi as PI\n",
        "from math import sqrt\n",
        "from typing import Callable, Dict, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding, Linear\n",
        "\n",
        "from torch_geometric.data import Dataset, download_url\n",
        "from torch_geometric.nn import radius_graph\n",
        "from torch_geometric.nn.inits import glorot_orthogonal\n",
        "from torch_geometric.nn.resolver import activation_resolver\n",
        "from torch_geometric.typing import OptTensor, SparseTensor\n",
        "from torch_geometric.utils import scatter\n",
        "\n",
        "qm9_target_dict: Dict[int, str] = {\n",
        "    0: 'mu',\n",
        "    1: 'alpha',\n",
        "    2: 'homo',\n",
        "    3: 'lumo',\n",
        "    5: 'r2',\n",
        "    6: 'zpve',\n",
        "    7: 'U0',\n",
        "    8: 'U',\n",
        "    9: 'H',\n",
        "    10: 'G',\n",
        "    11: 'Cv',\n",
        "}\n",
        "\n",
        "\n",
        "class Envelope(torch.nn.Module):\n",
        "    def __init__(self, exponent: int):\n",
        "        super().__init__()\n",
        "        self.p = exponent + 1\n",
        "        self.a = -(self.p + 1) * (self.p + 2) / 2\n",
        "        self.b = self.p * (self.p + 2)\n",
        "        self.c = -self.p * (self.p + 1) / 2\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        p, a, b, c = self.p, self.a, self.b, self.c\n",
        "        x_pow_p0 = x.pow(p - 1)\n",
        "        x_pow_p1 = x_pow_p0 * x\n",
        "        x_pow_p2 = x_pow_p1 * x\n",
        "        return (1.0 / x + a * x_pow_p0 + b * x_pow_p1 +\n",
        "                c * x_pow_p2) * (x < 1.0).to(x.dtype)\n",
        "\n",
        "\n",
        "class BesselBasisLayer(torch.nn.Module):\n",
        "    def __init__(self, num_radial: int, cutoff: float = 5.0,\n",
        "                 envelope_exponent: int = 5):\n",
        "        super().__init__()\n",
        "        self.cutoff = cutoff\n",
        "        self.envelope = Envelope(envelope_exponent)\n",
        "\n",
        "        self.freq = torch.nn.Parameter(torch.empty(num_radial))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        with torch.no_grad():\n",
        "            torch.arange(1, self.freq.numel() + 1, out=self.freq).mul_(PI)\n",
        "        self.freq.requires_grad_()\n",
        "\n",
        "    def forward(self, dist: Tensor) -> Tensor:\n",
        "        dist = dist.unsqueeze(-1) / self.cutoff\n",
        "        return self.envelope(dist) * (self.freq * dist).sin()\n",
        "\n",
        "\n",
        "class SphericalBasisLayer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        cutoff: float = 5.0,\n",
        "        envelope_exponent: int = 5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        import sympy as sym\n",
        "\n",
        "        from torch_geometric.nn.models.dimenet_utils import (\n",
        "            bessel_basis,\n",
        "            real_sph_harm,\n",
        "        )\n",
        "\n",
        "        assert num_radial <= 64\n",
        "        self.num_spherical = num_spherical\n",
        "        self.num_radial = num_radial\n",
        "        self.cutoff = cutoff\n",
        "        self.envelope = Envelope(envelope_exponent)\n",
        "\n",
        "        bessel_forms = bessel_basis(num_spherical, num_radial)\n",
        "        sph_harm_forms = real_sph_harm(num_spherical)\n",
        "        self.sph_funcs = []\n",
        "        self.bessel_funcs = []\n",
        "\n",
        "        x, theta = sym.symbols('x theta')\n",
        "        modules = {'sin': torch.sin, 'cos': torch.cos}\n",
        "        for i in range(num_spherical):\n",
        "            if i == 0:\n",
        "                sph1 = sym.lambdify([theta], sph_harm_forms[i][0], modules)(0)\n",
        "                self.sph_funcs.append(partial(self._sph_to_tensor, sph1))\n",
        "            else:\n",
        "                sph = sym.lambdify([theta], sph_harm_forms[i][0], modules)\n",
        "                self.sph_funcs.append(sph)\n",
        "            for j in range(num_radial):\n",
        "                bessel = sym.lambdify([x], bessel_forms[i][j], modules)\n",
        "                self.bessel_funcs.append(bessel)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sph_to_tensor(sph, x: Tensor) -> Tensor:\n",
        "        return torch.zeros_like(x) + sph\n",
        "\n",
        "    def forward(self, dist: Tensor, angle: Tensor, idx_kj: Tensor) -> Tensor:\n",
        "        dist = dist / self.cutoff\n",
        "        rbf = torch.stack([f(dist) for f in self.bessel_funcs], dim=1)\n",
        "        rbf = self.envelope(dist).unsqueeze(-1) * rbf\n",
        "\n",
        "        cbf = torch.stack([f(angle) for f in self.sph_funcs], dim=1)\n",
        "\n",
        "        n, k = self.num_spherical, self.num_radial\n",
        "        out = (rbf[idx_kj].view(-1, n, k) * cbf.view(-1, n, 1)).view(-1, n * k)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EmbeddingBlock(torch.nn.Module):\n",
        "    def __init__(self, num_radial: int, hidden_channels: int, act: Callable):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "\n",
        "        self.emb = Embedding(95, hidden_channels)\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels)\n",
        "        self.lin = Linear(3 * hidden_channels, hidden_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
        "        self.lin_rbf.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor, j: Tensor) -> Tensor:\n",
        "        x = self.emb(x)\n",
        "        rbf = self.act(self.lin_rbf(rbf))\n",
        "        return self.act(self.lin(torch.cat([x[i], x[j], rbf], dim=-1)))\n",
        "\n",
        "\n",
        "class ResidualLayer(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels: int, act: Callable):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin1.weight, scale=2.0)\n",
        "        self.lin1.bias.data.fill_(0)\n",
        "        glorot_orthogonal(self.lin2.weight, scale=2.0)\n",
        "        self.lin2.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return x + self.act(self.lin2(self.act(self.lin1(x))))\n",
        "\n",
        "\n",
        "class InteractionBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        num_bilinear: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        num_before_skip: int,\n",
        "        num_after_skip: int,\n",
        "        act: Callable,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
        "        self.lin_sbf = Linear(num_spherical * num_radial, num_bilinear,\n",
        "                              bias=False)\n",
        "\n",
        "        # Dense transformations of input messages.\n",
        "        self.lin_kj = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.W = torch.nn.Parameter(\n",
        "            torch.empty(hidden_channels, num_bilinear, hidden_channels))\n",
        "\n",
        "        self.layers_before_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
        "        ])\n",
        "        self.lin = Linear(hidden_channels, hidden_channels)\n",
        "        self.layers_after_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_after_skip)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_sbf.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
        "        self.lin_kj.bias.data.fill_(0)\n",
        "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
        "        self.lin_ji.bias.data.fill_(0)\n",
        "        self.W.data.normal_(mean=0, std=2 / self.W.size(0))\n",
        "        for res_layer in self.layers_before_skip:\n",
        "            res_layer.reset_parameters()\n",
        "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "        self.lin.bias.data.fill_(0)\n",
        "        for res_layer in self.layers_after_skip:\n",
        "            res_layer.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
        "                idx_ji: Tensor) -> Tensor:\n",
        "        rbf = self.lin_rbf(rbf)\n",
        "        sbf = self.lin_sbf(sbf)\n",
        "\n",
        "        x_ji = self.act(self.lin_ji(x))\n",
        "        x_kj = self.act(self.lin_kj(x))\n",
        "        x_kj = x_kj * rbf\n",
        "        x_kj = torch.einsum('wj,wl,ijl->wi', sbf, x_kj[idx_kj], self.W)\n",
        "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0), reduce='sum')\n",
        "\n",
        "        h = x_ji + x_kj\n",
        "        for layer in self.layers_before_skip:\n",
        "            h = layer(h)\n",
        "        h = self.act(self.lin(h)) + x\n",
        "        for layer in self.layers_after_skip:\n",
        "            h = layer(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class InteractionPPBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        int_emb_size: int,\n",
        "        basis_emb_size: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        num_before_skip: int,\n",
        "        num_after_skip: int,\n",
        "        act: Callable,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.act = act\n",
        "\n",
        "        # Transformation of Bessel and spherical basis representations:\n",
        "        self.lin_rbf1 = Linear(num_radial, basis_emb_size, bias=False)\n",
        "        self.lin_rbf2 = Linear(basis_emb_size, hidden_channels, bias=False)\n",
        "\n",
        "        self.lin_sbf1 = Linear(num_spherical * num_radial, basis_emb_size,\n",
        "                               bias=False)\n",
        "        self.lin_sbf2 = Linear(basis_emb_size, int_emb_size, bias=False)\n",
        "\n",
        "        # Hidden transformation of input message:\n",
        "        self.lin_kj = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin_ji = Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Embedding projections for interaction triplets:\n",
        "        self.lin_down = Linear(hidden_channels, int_emb_size, bias=False)\n",
        "        self.lin_up = Linear(int_emb_size, hidden_channels, bias=False)\n",
        "\n",
        "        # Residual layers before and after skip connection:\n",
        "        self.layers_before_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_before_skip)\n",
        "        ])\n",
        "        self.lin = Linear(hidden_channels, hidden_channels)\n",
        "        self.layers_after_skip = torch.nn.ModuleList([\n",
        "            ResidualLayer(hidden_channels, act) for _ in range(num_after_skip)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf1.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_rbf2.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_sbf1.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_sbf2.weight, scale=2.0)\n",
        "\n",
        "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
        "        self.lin_kj.bias.data.fill_(0)\n",
        "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
        "        self.lin_ji.bias.data.fill_(0)\n",
        "\n",
        "        glorot_orthogonal(self.lin_down.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
        "\n",
        "        for res_layer in self.layers_before_skip:\n",
        "            res_layer.reset_parameters()\n",
        "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "        self.lin.bias.data.fill_(0)\n",
        "        for res_layer in self.layers_after_skip:\n",
        "            res_layer.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, sbf: Tensor, idx_kj: Tensor,\n",
        "                idx_ji: Tensor) -> Tensor:\n",
        "        # Initial transformation:\n",
        "        x_ji = self.act(self.lin_ji(x))\n",
        "        x_kj = self.act(self.lin_kj(x))\n",
        "\n",
        "        # Transformation via Bessel basis:\n",
        "        rbf = self.lin_rbf1(rbf)\n",
        "        rbf = self.lin_rbf2(rbf)\n",
        "        x_kj = x_kj * rbf\n",
        "\n",
        "        # Down project embedding and generating triple-interactions:\n",
        "        x_kj = self.act(self.lin_down(x_kj))\n",
        "\n",
        "        # Transform via 2D spherical basis:\n",
        "        sbf = self.lin_sbf1(sbf)\n",
        "        sbf = self.lin_sbf2(sbf)\n",
        "        x_kj = x_kj[idx_kj] * sbf\n",
        "\n",
        "        # Aggregate interactions and up-project embeddings:\n",
        "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x.size(0), reduce='sum')\n",
        "        x_kj = self.act(self.lin_up(x_kj))\n",
        "\n",
        "        h = x_ji + x_kj\n",
        "        for layer in self.layers_before_skip:\n",
        "            h = layer(h)\n",
        "        h = self.act(self.lin(h)) + x\n",
        "        for layer in self.layers_after_skip:\n",
        "            h = layer(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class OutputBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_radial: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        num_layers: int,\n",
        "        act: Callable,\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        assert output_initializer in {'zeros', 'glorot_orthogonal'}\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.act = act\n",
        "        self.output_initializer = output_initializer\n",
        "\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.lins.append(Linear(hidden_channels, hidden_channels))\n",
        "        self.lin = Linear(hidden_channels, out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
        "        for lin in self.lins:\n",
        "            glorot_orthogonal(lin.weight, scale=2.0)\n",
        "            lin.bias.data.fill_(0)\n",
        "        if self.output_initializer == 'zeros':\n",
        "            self.lin.weight.data.fill_(0)\n",
        "        elif self.output_initializer == 'glorot_orthogonal':\n",
        "            glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
        "                num_nodes: Optional[int] = None) -> Tensor:\n",
        "        x = self.lin_rbf(rbf) * x\n",
        "        x = scatter(x, i, dim=0, dim_size=num_nodes, reduce='sum')\n",
        "        for lin in self.lins:\n",
        "            x = self.act(lin(x))\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "class OutputPPBlock(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_radial: int,\n",
        "        hidden_channels: int,\n",
        "        out_emb_channels: int,\n",
        "        out_channels: int,\n",
        "        num_layers: int,\n",
        "        act: Callable,\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        assert output_initializer in {'zeros', 'glorot_orthogonal'}\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.act = act\n",
        "        self.output_initializer = output_initializer\n",
        "\n",
        "        self.lin_rbf = Linear(num_radial, hidden_channels, bias=False)\n",
        "\n",
        "        # The up-projection layer:\n",
        "        self.lin_up = Linear(hidden_channels, out_emb_channels, bias=False)\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.lins.append(Linear(out_emb_channels, out_emb_channels))\n",
        "        self.lin = Linear(out_emb_channels, out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
        "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
        "        for lin in self.lins:\n",
        "            glorot_orthogonal(lin.weight, scale=2.0)\n",
        "            lin.bias.data.fill_(0)\n",
        "        if self.output_initializer == 'zeros':\n",
        "            self.lin.weight.data.fill_(0)\n",
        "        elif self.output_initializer == 'glorot_orthogonal':\n",
        "            glorot_orthogonal(self.lin.weight, scale=2.0)\n",
        "\n",
        "    def forward(self, x: Tensor, rbf: Tensor, i: Tensor,\n",
        "                num_nodes: Optional[int] = None) -> Tensor:\n",
        "        x = self.lin_rbf(rbf) * x\n",
        "        x = scatter(x, i, dim=0, dim_size=num_nodes, reduce='sum')\n",
        "        x = self.lin_up(x)\n",
        "        for lin in self.lins:\n",
        "            x = self.act(lin(x))\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "def triplets(\n",
        "    edge_index: Tensor,\n",
        "    num_nodes: int,\n",
        ") -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
        "    row, col = edge_index  # j->i\n",
        "\n",
        "    value = torch.arange(row.size(0), device=row.device)\n",
        "    adj_t = SparseTensor(row=col, col=row, value=value,\n",
        "                         sparse_sizes=(num_nodes, num_nodes))\n",
        "    adj_t_row = adj_t[row]\n",
        "    num_triplets = adj_t_row.set_value(None).sum(dim=1).to(torch.long)\n",
        "\n",
        "    # Node indices (k->j->i) for triplets.\n",
        "    idx_i = col.repeat_interleave(num_triplets)\n",
        "    idx_j = row.repeat_interleave(num_triplets)\n",
        "    idx_k = adj_t_row.storage.col()\n",
        "    mask = idx_i != idx_k  # Remove i == k triplets.\n",
        "    idx_i, idx_j, idx_k = idx_i[mask], idx_j[mask], idx_k[mask]\n",
        "\n",
        "    # Edge indices (k-j, j->i) for triplets.\n",
        "    idx_kj = adj_t_row.storage.value()[mask]\n",
        "    idx_ji = adj_t_row.storage.row()[mask]\n",
        "\n",
        "    return col, row, idx_i, idx_j, idx_k, idx_kj, idx_ji\n",
        "\n",
        "\n",
        "[docs]class DimeNet(torch.nn.Module):\n",
        "    r\"\"\"The directional message passing neural network (DimeNet) from the\n",
        "    `\"Directional Message Passing for Molecular Graphs\"\n",
        "    <https://arxiv.org/abs/2003.03123>`_ paper.\n",
        "    DimeNet transforms messages based on the angle between them in a\n",
        "    rotation-equivariant fashion.\n",
        "\n",
        "    .. note::\n",
        "\n",
        "        For an example of using a pretrained DimeNet variant, see\n",
        "        `examples/qm9_pretrained_dimenet.py\n",
        "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
        "        qm9_pretrained_dimenet.py>`_.\n",
        "\n",
        "    Args:\n",
        "        hidden_channels (int): Hidden embedding size.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_blocks (int): Number of building blocks.\n",
        "        num_bilinear (int): Size of the bilinear layer tensor.\n",
        "        num_spherical (int): Number of spherical harmonics.\n",
        "        num_radial (int): Number of radial basis functions.\n",
        "        cutoff (float, optional): Cutoff distance for interatomic\n",
        "            interactions. (default: :obj:`5.0`)\n",
        "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
        "            collect for each node within the :attr:`cutoff` distance.\n",
        "            (default: :obj:`32`)\n",
        "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
        "            (default: :obj:`5`)\n",
        "        num_before_skip (int, optional): Number of residual layers in the\n",
        "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
        "        num_after_skip (int, optional): Number of residual layers in the\n",
        "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
        "        num_output_layers (int, optional): Number of linear layers for the\n",
        "            output blocks. (default: :obj:`3`)\n",
        "        act (str or Callable, optional): The activation function.\n",
        "            (default: :obj:`\"swish\"`)\n",
        "        output_initializer (str, optional): The initialization method for the\n",
        "            output layer (:obj:`\"zeros\"`, :obj:`\"glorot_orthogonal\"`).\n",
        "            (default: :obj:`\"zeros\"`)\n",
        "    \"\"\"\n",
        "\n",
        "    url = ('https://github.com/klicperajo/dimenet/raw/master/pretrained/'\n",
        "           'dimenet')\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        num_blocks: int,\n",
        "        num_bilinear: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        cutoff: float = 5.0,\n",
        "        max_num_neighbors: int = 32,\n",
        "        envelope_exponent: int = 5,\n",
        "        num_before_skip: int = 1,\n",
        "        num_after_skip: int = 2,\n",
        "        num_output_layers: int = 3,\n",
        "        act: Union[str, Callable] = 'swish',\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if num_spherical < 2:\n",
        "            raise ValueError(\"'num_spherical' should be greater than 1\")\n",
        "\n",
        "        act = activation_resolver(act)\n",
        "\n",
        "        self.cutoff = cutoff\n",
        "        self.max_num_neighbors = max_num_neighbors\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        self.rbf = BesselBasisLayer(num_radial, cutoff, envelope_exponent)\n",
        "        self.sbf = SphericalBasisLayer(num_spherical, num_radial, cutoff,\n",
        "                                       envelope_exponent)\n",
        "\n",
        "        self.emb = EmbeddingBlock(num_radial, hidden_channels, act)\n",
        "\n",
        "        self.output_blocks = torch.nn.ModuleList([\n",
        "            OutputBlock(\n",
        "                num_radial,\n",
        "                hidden_channels,\n",
        "                out_channels,\n",
        "                num_output_layers,\n",
        "                act,\n",
        "                output_initializer,\n",
        "            ) for _ in range(num_blocks + 1)\n",
        "        ])\n",
        "\n",
        "        self.interaction_blocks = torch.nn.ModuleList([\n",
        "            InteractionBlock(\n",
        "                hidden_channels,\n",
        "                num_bilinear,\n",
        "                num_spherical,\n",
        "                num_radial,\n",
        "                num_before_skip,\n",
        "                num_after_skip,\n",
        "                act,\n",
        "            ) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "[docs]    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        self.rbf.reset_parameters()\n",
        "        self.emb.reset_parameters()\n",
        "        for out in self.output_blocks:\n",
        "            out.reset_parameters()\n",
        "        for interaction in self.interaction_blocks:\n",
        "            interaction.reset_parameters()\n",
        "\n",
        "[docs]    @classmethod\n",
        "    def from_qm9_pretrained(\n",
        "        cls,\n",
        "        root: str,\n",
        "        dataset: Dataset,\n",
        "        target: int,\n",
        "    ) -> Tuple['DimeNet', Dataset, Dataset, Dataset]:  # pragma: no cover\n",
        "        r\"\"\"Returns a pre-trained :class:`DimeNet` model on the\n",
        "        :class:`~torch_geometric.datasets.QM9` dataset, trained on the\n",
        "        specified target :obj:`target`.\n",
        "        \"\"\"\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "        import tensorflow as tf\n",
        "\n",
        "        assert target >= 0 and target <= 12 and not target == 4\n",
        "\n",
        "        root = osp.expanduser(osp.normpath(root))\n",
        "        path = osp.join(root, 'pretrained_dimenet', qm9_target_dict[target])\n",
        "\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        url = f'{cls.url}/{qm9_target_dict[target]}'\n",
        "\n",
        "        if not osp.exists(osp.join(path, 'checkpoint')):\n",
        "            download_url(f'{url}/checkpoint', path)\n",
        "            download_url(f'{url}/ckpt.data-00000-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.data-00001-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.index', path)\n",
        "\n",
        "        path = osp.join(path, 'ckpt')\n",
        "        reader = tf.train.load_checkpoint(path)\n",
        "\n",
        "        model = cls(\n",
        "            hidden_channels=128,\n",
        "            out_channels=1,\n",
        "            num_blocks=6,\n",
        "            num_bilinear=8,\n",
        "            num_spherical=7,\n",
        "            num_radial=6,\n",
        "            cutoff=5.0,\n",
        "            envelope_exponent=5,\n",
        "            num_before_skip=1,\n",
        "            num_after_skip=2,\n",
        "            num_output_layers=3,\n",
        "        )\n",
        "\n",
        "        def copy_(src, name, transpose=False):\n",
        "            init = reader.get_tensor(f'{name}/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "            init = torch.from_numpy(init)\n",
        "            if name[-6:] == 'kernel':\n",
        "                init = init.t()\n",
        "            src.data.copy_(init)\n",
        "\n",
        "        copy_(model.rbf.freq, 'rbf_layer/frequencies')\n",
        "        copy_(model.emb.emb.weight, 'emb_block/embeddings')\n",
        "        copy_(model.emb.lin_rbf.weight, 'emb_block/dense_rbf/kernel')\n",
        "        copy_(model.emb.lin_rbf.bias, 'emb_block/dense_rbf/bias')\n",
        "        copy_(model.emb.lin.weight, 'emb_block/dense/kernel')\n",
        "        copy_(model.emb.lin.bias, 'emb_block/dense/bias')\n",
        "\n",
        "        for i, block in enumerate(model.output_blocks):\n",
        "            copy_(block.lin_rbf.weight, f'output_blocks/{i}/dense_rbf/kernel')\n",
        "            for j, lin in enumerate(block.lins):\n",
        "                copy_(lin.weight, f'output_blocks/{i}/dense_layers/{j}/kernel')\n",
        "                copy_(lin.bias, f'output_blocks/{i}/dense_layers/{j}/bias')\n",
        "            copy_(block.lin.weight, f'output_blocks/{i}/dense_final/kernel')\n",
        "\n",
        "        for i, block in enumerate(model.interaction_blocks):\n",
        "            copy_(block.lin_rbf.weight, f'int_blocks/{i}/dense_rbf/kernel')\n",
        "            copy_(block.lin_sbf.weight, f'int_blocks/{i}/dense_sbf/kernel')\n",
        "            copy_(block.lin_kj.weight, f'int_blocks/{i}/dense_kj/kernel')\n",
        "            copy_(block.lin_kj.bias, f'int_blocks/{i}/dense_kj/bias')\n",
        "            copy_(block.lin_ji.weight, f'int_blocks/{i}/dense_ji/kernel')\n",
        "            copy_(block.lin_ji.bias, f'int_blocks/{i}/dense_ji/bias')\n",
        "            copy_(block.W, f'int_blocks/{i}/bilinear')\n",
        "            for j, layer in enumerate(block.layers_before_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/bias')\n",
        "            copy_(block.lin.weight, f'int_blocks/{i}/final_before_skip/kernel')\n",
        "            copy_(block.lin.bias, f'int_blocks/{i}/final_before_skip/bias')\n",
        "            for j, layer in enumerate(block.layers_after_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/bias')\n",
        "\n",
        "        # Use the same random seed as the official DimeNet` implementation.\n",
        "        random_state = np.random.RandomState(seed=42)\n",
        "        perm = torch.from_numpy(random_state.permutation(np.arange(130831)))\n",
        "        perm = perm.long()\n",
        "        train_idx = perm[:110000]\n",
        "        val_idx = perm[110000:120000]\n",
        "        test_idx = perm[120000:]\n",
        "\n",
        "        return model, (dataset[train_idx], dataset[val_idx], dataset[test_idx])\n",
        "\n",
        "[docs]    def forward(\n",
        "        self,\n",
        "        z: Tensor,\n",
        "        pos: Tensor,\n",
        "        batch: OptTensor = None,\n",
        "    ) -> Tensor:\n",
        "        r\"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            z (torch.Tensor): Atomic number of each atom with shape\n",
        "                :obj:`[num_atoms]`.\n",
        "            pos (torch.Tensor): Coordinates of each atom with shape\n",
        "                :obj:`[num_atoms, 3]`.\n",
        "            batch (torch.Tensor, optional): Batch indices assigning each atom\n",
        "                to a separate molecule with shape :obj:`[num_atoms]`.\n",
        "                (default: :obj:`None`)\n",
        "        \"\"\"\n",
        "        edge_index = radius_graph(pos, r=self.cutoff, batch=batch,\n",
        "                                  max_num_neighbors=self.max_num_neighbors)\n",
        "\n",
        "        i, j, idx_i, idx_j, idx_k, idx_kj, idx_ji = triplets(\n",
        "            edge_index, num_nodes=z.size(0))\n",
        "\n",
        "        # Calculate distances.\n",
        "        dist = (pos[i] - pos[j]).pow(2).sum(dim=-1).sqrt()\n",
        "\n",
        "        # Calculate angles.\n",
        "        if isinstance(self, DimeNetPlusPlus):\n",
        "            pos_jk, pos_ij = pos[idx_j] - pos[idx_k], pos[idx_i] - pos[idx_j]\n",
        "            a = (pos_ij * pos_jk).sum(dim=-1)\n",
        "            b = torch.cross(pos_ij, pos_jk).norm(dim=-1)\n",
        "        elif isinstance(self, DimeNet):\n",
        "            pos_ji, pos_ki = pos[idx_j] - pos[idx_i], pos[idx_k] - pos[idx_i]\n",
        "            a = (pos_ji * pos_ki).sum(dim=-1)\n",
        "            b = torch.cross(pos_ji, pos_ki).norm(dim=-1)\n",
        "        angle = torch.atan2(b, a)\n",
        "\n",
        "        rbf = self.rbf(dist)\n",
        "        sbf = self.sbf(dist, angle, idx_kj)\n",
        "\n",
        "        # Embedding block.\n",
        "        x = self.emb(z, rbf, i, j)\n",
        "        P = self.output_blocks[0](x, rbf, i, num_nodes=pos.size(0))\n",
        "\n",
        "        # Interaction blocks.\n",
        "        for interaction_block, output_block in zip(self.interaction_blocks,\n",
        "                                                   self.output_blocks[1:]):\n",
        "            x = interaction_block(x, rbf, sbf, idx_kj, idx_ji)\n",
        "            P = P + output_block(x, rbf, i, num_nodes=pos.size(0))\n",
        "\n",
        "        if batch is None:\n",
        "            return P.sum(dim=0)\n",
        "        else:\n",
        "            return scatter(P, batch, dim=0, reduce='sum')\n",
        "\n",
        "\n",
        "[docs]class DimeNetPlusPlus(DimeNet):\n",
        "    r\"\"\"The DimeNet++ from the `\"Fast and Uncertainty-Aware\n",
        "    Directional Message Passing for Non-Equilibrium Molecules\"\n",
        "    <https://arxiv.org/abs/2011.14115>`_ paper.\n",
        "\n",
        "    :class:`DimeNetPlusPlus` is an upgrade to the :class:`DimeNet` model with\n",
        "    8x faster and 10% more accurate than :class:`DimeNet`.\n",
        "\n",
        "    Args:\n",
        "        hidden_channels (int): Hidden embedding size.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_blocks (int): Number of building blocks.\n",
        "        int_emb_size (int): Size of embedding in the interaction block.\n",
        "        basis_emb_size (int): Size of basis embedding in the interaction block.\n",
        "        out_emb_channels (int): Size of embedding in the output block.\n",
        "        num_spherical (int): Number of spherical harmonics.\n",
        "        num_radial (int): Number of radial basis functions.\n",
        "        cutoff: (float, optional): Cutoff distance for interatomic\n",
        "            interactions. (default: :obj:`5.0`)\n",
        "        max_num_neighbors (int, optional): The maximum number of neighbors to\n",
        "            collect for each node within the :attr:`cutoff` distance.\n",
        "            (default: :obj:`32`)\n",
        "        envelope_exponent (int, optional): Shape of the smooth cutoff.\n",
        "            (default: :obj:`5`)\n",
        "        num_before_skip: (int, optional): Number of residual layers in the\n",
        "            interaction blocks before the skip connection. (default: :obj:`1`)\n",
        "        num_after_skip: (int, optional): Number of residual layers in the\n",
        "            interaction blocks after the skip connection. (default: :obj:`2`)\n",
        "        num_output_layers: (int, optional): Number of linear layers for the\n",
        "            output blocks. (default: :obj:`3`)\n",
        "        act: (str or Callable, optional): The activation funtion.\n",
        "            (default: :obj:`\"swish\"`)\n",
        "        output_initializer (str, optional): The initialization method for the\n",
        "            output layer (:obj:`\"zeros\"`, :obj:`\"glorot_orthogonal\"`).\n",
        "            (default: :obj:`\"zeros\"`)\n",
        "    \"\"\"\n",
        "\n",
        "    url = ('https://raw.githubusercontent.com/gasteigerjo/dimenet/'\n",
        "           'master/pretrained/dimenet_pp')\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        num_blocks: int,\n",
        "        int_emb_size: int,\n",
        "        basis_emb_size: int,\n",
        "        out_emb_channels: int,\n",
        "        num_spherical: int,\n",
        "        num_radial: int,\n",
        "        cutoff: float = 5.0,\n",
        "        max_num_neighbors: int = 32,\n",
        "        envelope_exponent: int = 5,\n",
        "        num_before_skip: int = 1,\n",
        "        num_after_skip: int = 2,\n",
        "        num_output_layers: int = 3,\n",
        "        act: Union[str, Callable] = 'swish',\n",
        "        output_initializer: str = 'zeros',\n",
        "    ):\n",
        "        act = activation_resolver(act)\n",
        "\n",
        "        super().__init__(\n",
        "            hidden_channels=hidden_channels,\n",
        "            out_channels=out_channels,\n",
        "            num_blocks=num_blocks,\n",
        "            num_bilinear=1,\n",
        "            num_spherical=num_spherical,\n",
        "            num_radial=num_radial,\n",
        "            cutoff=cutoff,\n",
        "            max_num_neighbors=max_num_neighbors,\n",
        "            envelope_exponent=envelope_exponent,\n",
        "            num_before_skip=num_before_skip,\n",
        "            num_after_skip=num_after_skip,\n",
        "            num_output_layers=num_output_layers,\n",
        "            act=act,\n",
        "            output_initializer=output_initializer,\n",
        "        )\n",
        "\n",
        "        # We are re-using the RBF, SBF and embedding layers of `DimeNet` and\n",
        "        # redefine output_block and interaction_block in DimeNet++.\n",
        "        # Hence, it is to be noted that in the above initalization, the\n",
        "        # variable `num_bilinear` does not have any purpose as it is used\n",
        "        # solely in the `OutputBlock` of DimeNet:\n",
        "        self.output_blocks = torch.nn.ModuleList([\n",
        "            OutputPPBlock(\n",
        "                num_radial,\n",
        "                hidden_channels,\n",
        "                out_emb_channels,\n",
        "                out_channels,\n",
        "                num_output_layers,\n",
        "                act,\n",
        "                output_initializer,\n",
        "            ) for _ in range(num_blocks + 1)\n",
        "        ])\n",
        "\n",
        "        self.interaction_blocks = torch.nn.ModuleList([\n",
        "            InteractionPPBlock(\n",
        "                hidden_channels,\n",
        "                int_emb_size,\n",
        "                basis_emb_size,\n",
        "                num_spherical,\n",
        "                num_radial,\n",
        "                num_before_skip,\n",
        "                num_after_skip,\n",
        "                act,\n",
        "            ) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "[docs]    @classmethod\n",
        "    def from_qm9_pretrained(\n",
        "        cls,\n",
        "        root: str,\n",
        "        dataset: Dataset,\n",
        "        target: int,\n",
        "    ) -> Tuple['DimeNetPlusPlus', Dataset, Dataset,\n",
        "               Dataset]:  # pragma: no cover\n",
        "        r\"\"\"Returns a pre-trained :class:`DimeNetPlusPlus` model on the\n",
        "        :class:`~torch_geometric.datasets.QM9` dataset, trained on the\n",
        "        specified target :obj:`target`.\n",
        "        \"\"\"\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "        import tensorflow as tf\n",
        "\n",
        "        assert target >= 0 and target <= 12 and not target == 4\n",
        "\n",
        "        root = osp.expanduser(osp.normpath(root))\n",
        "        path = osp.join(root, 'pretrained_dimenet_pp', qm9_target_dict[target])\n",
        "\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        url = f'{cls.url}/{qm9_target_dict[target]}'\n",
        "\n",
        "        if not osp.exists(osp.join(path, 'checkpoint')):\n",
        "            download_url(f'{url}/checkpoint', path)\n",
        "            download_url(f'{url}/ckpt.data-00000-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.data-00001-of-00002', path)\n",
        "            download_url(f'{url}/ckpt.index', path)\n",
        "\n",
        "        path = osp.join(path, 'ckpt')\n",
        "        reader = tf.train.load_checkpoint(path)\n",
        "\n",
        "        # Configuration from DimeNet++:\n",
        "        # https://github.com/gasteigerjo/dimenet/blob/master/config_pp.yaml\n",
        "        model = cls(\n",
        "            hidden_channels=128,\n",
        "            out_channels=1,\n",
        "            num_blocks=4,\n",
        "            int_emb_size=64,\n",
        "            basis_emb_size=8,\n",
        "            out_emb_channels=256,\n",
        "            num_spherical=7,\n",
        "            num_radial=6,\n",
        "            cutoff=5.0,\n",
        "            max_num_neighbors=32,\n",
        "            envelope_exponent=5,\n",
        "            num_before_skip=1,\n",
        "            num_after_skip=2,\n",
        "            num_output_layers=3,\n",
        "        )\n",
        "\n",
        "        def copy_(src, name, transpose=False):\n",
        "            init = reader.get_tensor(f'{name}/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "            init = torch.from_numpy(init)\n",
        "            if name[-6:] == 'kernel':\n",
        "                init = init.t()\n",
        "            src.data.copy_(init)\n",
        "\n",
        "        copy_(model.rbf.freq, 'rbf_layer/frequencies')\n",
        "        copy_(model.emb.emb.weight, 'emb_block/embeddings')\n",
        "        copy_(model.emb.lin_rbf.weight, 'emb_block/dense_rbf/kernel')\n",
        "        copy_(model.emb.lin_rbf.bias, 'emb_block/dense_rbf/bias')\n",
        "        copy_(model.emb.lin.weight, 'emb_block/dense/kernel')\n",
        "        copy_(model.emb.lin.bias, 'emb_block/dense/bias')\n",
        "\n",
        "        for i, block in enumerate(model.output_blocks):\n",
        "            copy_(block.lin_rbf.weight, f'output_blocks/{i}/dense_rbf/kernel')\n",
        "            copy_(block.lin_up.weight,\n",
        "                  f'output_blocks/{i}/up_projection/kernel')\n",
        "            for j, lin in enumerate(block.lins):\n",
        "                copy_(lin.weight, f'output_blocks/{i}/dense_layers/{j}/kernel')\n",
        "                copy_(lin.bias, f'output_blocks/{i}/dense_layers/{j}/bias')\n",
        "            copy_(block.lin.weight, f'output_blocks/{i}/dense_final/kernel')\n",
        "\n",
        "        for i, block in enumerate(model.interaction_blocks):\n",
        "            copy_(block.lin_rbf1.weight, f'int_blocks/{i}/dense_rbf1/kernel')\n",
        "            copy_(block.lin_rbf2.weight, f'int_blocks/{i}/dense_rbf2/kernel')\n",
        "            copy_(block.lin_sbf1.weight, f'int_blocks/{i}/dense_sbf1/kernel')\n",
        "            copy_(block.lin_sbf2.weight, f'int_blocks/{i}/dense_sbf2/kernel')\n",
        "\n",
        "            copy_(block.lin_ji.weight, f'int_blocks/{i}/dense_ji/kernel')\n",
        "            copy_(block.lin_ji.bias, f'int_blocks/{i}/dense_ji/bias')\n",
        "            copy_(block.lin_kj.weight, f'int_blocks/{i}/dense_kj/kernel')\n",
        "            copy_(block.lin_kj.bias, f'int_blocks/{i}/dense_kj/bias')\n",
        "\n",
        "            copy_(block.lin_down.weight,\n",
        "                  f'int_blocks/{i}/down_projection/kernel')\n",
        "            copy_(block.lin_up.weight, f'int_blocks/{i}/up_projection/kernel')\n",
        "\n",
        "            for j, layer in enumerate(block.layers_before_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_before_skip/{j}/dense_2/bias')\n",
        "\n",
        "            copy_(block.lin.weight, f'int_blocks/{i}/final_before_skip/kernel')\n",
        "            copy_(block.lin.bias, f'int_blocks/{i}/final_before_skip/bias')\n",
        "\n",
        "            for j, layer in enumerate(block.layers_after_skip):\n",
        "                copy_(layer.lin1.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/kernel')\n",
        "                copy_(layer.lin1.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_1/bias')\n",
        "                copy_(layer.lin2.weight,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/kernel')\n",
        "                copy_(layer.lin2.bias,\n",
        "                      f'int_blocks/{i}/layers_after_skip/{j}/dense_2/bias')\n",
        "\n",
        "        random_state = np.random.RandomState(seed=42)\n",
        "        perm = torch.from_numpy(random_state.permutation(np.arange(130831)))\n",
        "        perm = perm.long()\n",
        "        train_idx = perm[:110000]\n",
        "        val_idx = perm[110000:120000]\n",
        "        test_idx = perm[120000:]\n",
        "\n",
        "        return model, (dataset[train_idx], dataset[val_idx], dataset[test_idx])"
      ]
    }
  ]
}